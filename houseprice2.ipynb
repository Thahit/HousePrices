{"cells":[{"metadata":{},"cell_type":"markdown","source":"# House Price Prediction2\nI decided to Fork the Notebook and  change some of the features.(log of saleprice).\nScaling the Saleprice seems to have a very positive impact on the acc. of the models.\n\n# Imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\n\n# plot\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import  norm\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.preprocessing import RobustScaler \n#This Scaler removes the median and scales the data according to the quantile range\n# better for models like SVR\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# import warnings \n# warnings.filterwarnings('ignore')\n\nprint(\"imports sucessful\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Processing"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train=pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\", index_col=\"Id\")\ntest=pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\", index_col=\"Id\")\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train.shape: \", train.shape, \"\\tTest.shape: \", test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainY=train.pop(\"SalePrice\")\ntrainY","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainLen=len(train)\nallData=pd.concat((train,test)).reset_index(drop=True)\nallData","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We first want to find and get rid of NaN"},{"metadata":{"trusted":true},"cell_type":"code","source":"objCols = allData.select_dtypes(include=['object']).columns\nnumCols =allData.select_dtypes(exclude=['object']).columns\nobjCols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numCols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"len num: \", len(numCols),\"\\tlen num: \", len(objCols))  #a+b=79   so check if we have all in a category","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nan=[col for col in allData.columns if allData[col].isnull().sum()>0] #List of Cols with nan\nprint(nan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(allData['MasVnrType'].isnull().sum(),allData[\"MasVnrArea\"].isnull().sum()) # =no type=> no area? =>no mas..","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(allData['GarageType'].isnull().sum(),allData[\"GarageYrBlt\"].isnull().sum())  #similar => no garage=> no jear built","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"allData['KitchenQual'].isnull().sum()  # 1 missing   maybe no kitchen","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# a nan in fireplace. pool etc => no pool/fireplace etc.  Exterior2nd=Nan=probl just 1 material    no basement parameters=> probl no basement...\n# you can get information from the data_description.txt\n# these assumtions might be false sometime, but the results should be much better than dropping important cols like pool\ncat=[\"Fence\",\"PoolQC\", \"MiscFeature\", \"Alley\",'FireplaceQu', 'MasVnrType',\"Exterior2nd\", \"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\", \"BsmtFinType2\", \"KitchenQual\", \n   \"GarageType\", \"GarageFinish\", \"GarageQual\", 'GarageCond', \"BsmtQual\"]#fill with None\nnum=[\"BsmtFinSF1\",\"MasVnrArea\",\"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"BsmtFullBath\", \"BsmtHalfBath\", \"GarageYrBlt\", \"GarageCars\", \"GarageArea\", ] #fill with 0\nfor i in cat:\n    allData[i].fillna(value=\"None\", inplace=True)\nfor i in num:\n    allData[i].fillna(value=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nan=[col for col in allData.columns if allData[col].isnull().sum()>0] #List of Cols with nan   after filling in stuff\nprint(nan) # these are the Parameters that you cant fill as easily(as far as i can see)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in nan:\n    print(allData[i].isnull().sum())  # only LotFrontage has a lot of missing vals","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tiny=['MSZoning', 'Utilities', 'Exterior1st', 'Electrical', 'Functional','SaleType']\nallData[tiny] = allData[tiny].fillna(allData.mode().iloc[0])  #fill cat with most common","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nan=[col for col in allData.columns if allData[col].isnull().sum()>0] #List of Cols with nan   after filling in stuff\nnan # LotFrontage is numerical  = Linear feet of street connected to property\n#there are a lot of missing vals   I don't think this parametier is very Important to the price of a House but im not an expert","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"allData[\"LotFrontage\"] = allData[\"LotFrontage\"].fillna(allData[\"LotFrontage\"].mean())  #fill with mean","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now that we have gotten rid of NaN we have to further prep the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"numCols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"notes: <br>\n- Maybe we could change YrSold and similar parameters to sold x years ago, which could improve the performance, but I dont't know what time the data has been collected. I will use 2020 as the \"now\" time. \n- I decide not to combine month and year. Month on its own shows which season the sale took place which could have an impact."},{"metadata":{"trusted":true},"cell_type":"code","source":"# new cols, that might get insight/make it more obvious\nallData[\"SoldYrAgo\"]=2020-allData[\"YrSold\"]\nallData[\"GarageYrBltAgo\"]=2020-allData[\"GarageYrBlt\"]\nallData[\"BuiltAgo\"]=2020-allData[\"YearBuilt\"]\nallData[\"SoldYrAgo\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating the data to plot with later   before scaling... happens\nplotdata=allData[:trainLen]\nplotdata[\"Saleprice\"]=trainY","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#onehot cols that are not linear\n# i dont replace all categorical cols, because some should work fine in 1 col, i m looking at the data_description\nneededOneHot=[\"MSZoning\", \"MSSubClass\", \"LandContour\", \"Utilities\", \"LotConfig\", \"Neighborhood\", \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\", \"RoofStyle\", \n       \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"ExterCond\",  \"Foundation\",  \"Heating\", \"Electrical\", \"GarageType\", \"MiscFeature\",\n        \"SaleType\", \"SaleCondition\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"enc = OneHotEncoder(handle_unknown='ignore')\nfor col in neededOneHot:\n    allData=pd.concat([allData,pd.get_dummies(allData[col],prefix=col)], axis=1)\n    allData.drop([col],axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"allData.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"allData.BsmtFinType2.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# manually concerting \"quality\" parameters\ntrans = {\"Grvl\": 0, \"Pave\": 1, \"None\": 2,}\nallData['Alley']= allData['Alley'].map(trans)\n\ntrans = {\"Reg\": 0, \"IR1\": 1, \"IR2\": 2, \"IR3\": 3, \"None\": 4}\nallData['LotShape']= allData['LotShape'].map(trans)\n\ntrans = {\"Gtl\": 0, \"Mod\": 1, \"Sev\": 2, \"None\": 3}\nallData['LandSlope']= allData['LandSlope'].map(trans)\n\ntrans = {\"Ex\": 0, \"Gd\": 1, \"TA\": 2, \"Fa\": 3, \"Po\": 4, \"None\": 5}\nallData['ExterQual']= allData['ExterQual'].map(trans)\n\ntrans = {\"Ex\": 0, \"Gd\": 1, \"TA\": 2, \"Fa\": 3, \"Po\": 4,  \"None\": 5}\nallData['BsmtQual']= allData['BsmtQual'].map(trans)\n\ntrans = {\"Ex\": 0, \"Gd\": 1, \"TA\": 2, \"Fa\": 3, \"Po\": 4,  \"None\": 5}\nallData['BsmtCond']= allData['BsmtCond'].map(trans)\n\ntrans = { \"Gd\": 1, \"Av\": 2, \"Mn\": 3, \"No\": 4,  \"None\": 5}\nallData['BsmtExposure']= allData['BsmtExposure'].map(trans)\n\ntrans = {\"GLQ\": 0, \"ALQ\": 1, \"BLQ\": 2, \"Rec\": 3, \"LwQ\": 4,  \"Unf\": 5, \"None\": 6}\nallData['BsmtFinType1']= allData['BsmtFinType1'].map(trans)\n\ntrans = {\"GLQ\": 0, \"ALQ\": 1, \"BLQ\": 2, \"Rec\": 3, \"LwQ\": 4,  \"Unf\": 5, \"None\": 6}\nallData['BsmtFinType2']= allData['BsmtFinType2'].map(trans)\n\ntrans = {\"Ex\": 0, \"Gd\": 1, \"TA\": 2, \"Fa\": 3, \"Po\": 4, \"None\": 5}\nallData['HeatingQC']= allData['HeatingQC'].map(trans)\n\ntrans = {\"Ex\": 0, \"Gd\": 1, \"TA\": 2, \"Fa\": 3, \"Po\": 4, \"None\": 5}\nallData['KitchenQual']= allData['KitchenQual'].map(trans)\n\ntrans = {\"Typ\": 0, \"Min1\": 1, \"Min2\": 2, \"Mod\": 3, \"Maj1\": 4,  \"Maj2\": 5,\"Sev\": 6,\"Sal\": 7, \"None\": 8}\nallData['Functional']= allData['Functional'].map(trans)\n\ntrans = {\"Ex\": 0, \"Gd\": 1, \"TA\": 2, \"Fa\": 3, \"Po\": 4, \"None\": 5}\nallData['FireplaceQu']= allData['FireplaceQu'].map(trans)\n\ntrans = {\"Fin\": 0, \"RFn\": 1, \"Unf\": 2,  \"None\": 3}\nallData['GarageFinish']= allData['GarageFinish'].map(trans)\n\ntrans = {\"Ex\": 0, \"Gd\": 1, \"TA\": 2, \"Fa\": 3, \"Po\": 4, \"None\": 5}\nallData['GarageQual']= allData['GarageQual'].map(trans)\n\ntrans = {\"Ex\": 0, \"Gd\": 1, \"TA\": 2, \"Fa\": 3, \"Po\": 4, \"None\": 5}\nallData['GarageCond']= allData['GarageCond'].map(trans)\n\ntrans = {\"Ex\": 0, \"Gd\": 1, \"TA\": 2, \"Fa\": 3, \"Po\": 4, \"None\": 5}\nallData['PoolQC']= allData['PoolQC'].map(trans)\n\ntrans = {\"GdPrv\": 0, \"MnPrv\": 1, \"GdWo\": 2, \"MnWw\": 3,  \"None\": 5}\nallData['Fence']= allData['Fence'].map(trans)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#redo after onehot etc\nobjCols = allData.select_dtypes(include=['object']).columns\nnumCols =allData.select_dtypes(exclude=['object']).columns\nl=[]\nfor i in objCols: #showing cat data example\n    l.append(allData[i][0:5])\nprint(l)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#changing cat. data to num\nfor i in objCols:\n    allData[i]= allData[i].astype('category').cat.codes\n\nl=[]\nfor i in objCols: #showing they changed to num\n    l.append(allData[i][0:5])\nprint(l)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop the cols we have twice\nallData.drop(\"YrSold\", axis=1, inplace=True)\nallData.drop(\"GarageYrBltAgo\", axis=1, inplace=True)\nallData.drop(\"BuiltAgo\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"allData.shape #the shape stays the same","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler()\nfor col in allData.columns:\n    allData[col] = scaler.fit_transform(allData[col].values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"allData","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"notes:\nI don't think I should remove any col."},{"metadata":{"trusted":true},"cell_type":"code","source":"trainY","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# log of saleprice as new saleprice\ntrainY=np.log1p(trainY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainY","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot\nI don't know what I want to plot as the nr. of cols is overwhelming and I don't know much about House-Prices. I will edit the data if I see it is needed in here."},{"metadata":{"trusted":true},"cell_type":"code","source":"# #attention this will take a while\n# plt.figure(figsize=(64,48))\n# sns.heatmap(plotdata.corr(),annot=True,cmap='Reds')\n# plt.show()# a lot of small correlations","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen on the Heatmap, the correlations between the parameters and Saleprice are very small.<br>\nIt is also notable that YearBuilt and BuiltAgo for example have the same correlation, which is logical as they say the same thing"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotdata.columns#seel what I can plot with","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the distribution of some cols"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.distplot(plotdata['Saleprice'],bins=30,color='m', fit=norm)\nfig = plt.figure()\nres = stats.probplot(plotdata['Saleprice'], plot=plt)\n# prob. plot from https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# a lot of ml models work better with norm dist.\nplotdata[\"SalepriceLog\"] = np.log1p(plotdata[\"Saleprice\"])\n# allData[\"Saleprice\"] = np.log1p(allData[\"Saleprice\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.distplot(plotdata['SalepriceLog'],bins=30,color='m', fit=norm)\nfig = plt.figure()\nres = stats.probplot(plotdata['SalepriceLog'], plot=plt)# much better\n# need to np.expm1() the output to undo the log. for submitting results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))# not a used col\nsns.distplot(plotdata['BuiltAgo'],bins=30, fit=norm)# most houses are not that old","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# a lot of ml models work better with norm dist.\nplotdata[\"BuiltAgo\"] = np.log1p(plotdata[\"BuiltAgo\"])\nplt.figure(figsize=(8,5))\nsns.distplot(plotdata['BuiltAgo'],bins=30, fit=norm)# dont know if this is better as it has several bumps I dont change the allData","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.distplot(plotdata['YearBuilt'],bins=30, fit=norm,color='r')# most houses are not that old","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.distplot(plotdata['GarageArea'],bins=30, fit=norm,color='g')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotdata[\"GarageArea\"] = np.log1p(plotdata[\"GarageArea\"])\nplt.figure(figsize=(8,5))\nsns.distplot(plotdata['GarageArea'],bins=30, fit=norm,color='g')# much worse?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.distplot(plotdata['GrLivArea'],bins=30, fit=norm,color='m')# much worse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.scatterplot(x=\"GrLivArea\",y=\"Saleprice\",  data=plotdata)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.scatterplot(x=\"BuiltAgo\",y=\"Saleprice\",hue=\"Fireplaces\",  data=plotdata)# fireplaces are nice, but they are not that hard to build if the house does not have one already","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.scatterplot(x=\"MSZoning\",y=\"Saleprice\",  data=plotdata)# some are clearly \"cheaper\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.scatterplot(x=\"GrLivArea\",y=\"Saleprice\",  data=plotdata,color='m') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.scatterplot(x=\"GarageArea\",y=\"Saleprice\",hue=\"GarageCond\",  data=plotdata)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.scatterplot(x=\"GarageArea\",y=\"Saleprice\",hue=\"GarageQual\",  data=plotdata)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2, ax3, ax4) = plt.subplots(1,4,figsize=(15,5))\nplt.figure(figsize=(80,50))\nfig.suptitle('Garage')\nax1.scatter(x=\"GarageArea\",y=\"Saleprice\",  data=plotdata, linewidth=1)\nax2.scatter(x=\"GarageQual\",y=\"Saleprice\",  data=plotdata, linewidth=1)\nax3.scatter(x=\"GarageCond\",y=\"Saleprice\",  data=plotdata, linewidth=1)\nax4.scatter(x=\"GarageFinish\",y=\"Saleprice\",  data=plotdata, linewidth=1)\n\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.scatterplot(x=\"PoolArea\",y=\"Saleprice\",hue=\"PoolQC\",  data=plotdata)# a pool does not seem to make a house very expensive interestingly","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.scatterplot(x=\"SoldYrAgo\",y=\"Saleprice\", data=plotdata,color='g')#older sales=> more expensive  (bubble?)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Models\ntesting a lot of several models."},{"metadata":{"trusted":true},"cell_type":"code","source":"#final nan check\nnan=[col for col in allData.columns if allData[col].isnull().sum()>0] #List of Cols with nan   after filling in stuff\nprint(nan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data for the models\ntrainX=np.array(allData[:trainLen])\ntestX=np.array(allData[trainLen:])\ntrainY=np.array(trainY)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear regression"},{"metadata":{},"cell_type":"markdown","source":"there is a strange bug, wehre saleprice with index 1444 will return inf. But if I use the Robustscaler it works."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# modelLin = LinearRegression()\n# modelLin.fit(trainX, trainY)\n# predTrain=np.expm1(modelLin.predict(trainX))\n# LinAcc = mean_squared_error(predTrain, np.expm1(trainY))\n# print (str(LinAcc) + ' MSE') #Score= failed(bug?)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelLin = make_pipeline(RobustScaler(), LinearRegression())\nmodelLin.fit(trainX, trainY)\npredTrain=np.expm1(modelLin.predict(trainX))\nLinAcc = mean_squared_error(predTrain, np.expm1(trainY))\nprint (str(LinAcc) + ' MSE') #Score= 0.14579","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot results\nplt.figure(figsize=(12,5))\nsns.distplot((np.expm1(trainY)),bins=30, color='m')\nsns.distplot((predTrain),bins=30, color='g')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the score seems to be very bad"},{"metadata":{},"cell_type":"markdown","source":"# SVR"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"manually tunning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# kernels: poly, rbf\nmodelSvr = make_pipeline(RobustScaler(), SVR(kernel=\"poly\", C=1, degree=1))    # is (almost) like linReg when 1degree...\nmodelSvr.fit(trainX,trainY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predTrain=np.expm1(modelSvr.predict(trainX))\nSVRAcc = mean_squared_error(predTrain, np.expm1(trainY))\nprint (str(SVRAcc) + ' MSE') # huge mse   score=0.13235   #very diffrent than houseprice1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot results\nplt.figure(figsize=(12,5))\nsns.distplot((np.expm1(trainY)),bins=30, color='m')\nsns.distplot((predTrain),bins=30, color='g')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hypertunning 1<br>\nattention, the hypertunning may take a very long time(don't run this for your own sake)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# #xgboost\n# modelX=xgboost.XGBRegressor(verbose=2)\n\n# n_estimators = [500, 900, 2000] # a lot of cols=> many estim..\n# max_depth = [3, 6,9,12]\n# booster=['gbtree','gblinear'] #\"dart\"\n# learning_rate=[.02,.05,.1,.2]\n# min_child_weight=[2,5,9]\n# base_score=[.01,.25,.5,.75]\n# max_depth=[2,4,6,8]\n# min_split_loss=[0,0.01,.1]\n# hyperparameter_grid = {\n#     'n_estimators': n_estimators,\n#     'max_depth':max_depth,\n#     'learning_rate':learning_rate,\n#     'min_child_weight':min_child_weight,\n#     'booster':booster,\n#     'base_score':base_score,\n#     \"max_depth\":max_depth,\n#     \"min_split_loss\":min_split_loss}\n\n# random_cv = RandomizedSearchCV(estimator=modelX,\n#             param_distributions=hyperparameter_grid,\n#             cv=5, n_iter=100,\n#             scoring='neg_root_mean_squared_error',\n#                                n_jobs = -1,\n#             verbose = 5, \n#             return_train_score = True,\n#             random_state=42)\n\n# random_cv.fit(trainX,trainY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# random_cv.best_estimator_    #if best estim are at the edge=> additional tests","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# modelX=xgboost.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n#              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n#              importance_type='gain', interaction_constraints='',\n#              learning_rate=0.02, max_delta_step=0, max_depth=8,\n#              min_child_weight=9, min_split_loss=0, missing=None,\n#              monotone_constraints='()', n_estimators=2000, n_jobs=0,\n#              num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1,\n#              scale_pos_weight=1, subsample=1, tree_method='exact',\n#              validate_parameters=1,  verbosity=None)#Score= 0.12949","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# modelX=xgboost.XGBRegressor(base_score=0.01, booster='gbtree', colsample_bylevel=1,\n#              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n#              importance_type='gain', interaction_constraints='',\n#              learning_rate=0.05, max_delta_step=0, max_depth=4,\n#              min_child_weight=5, min_split_loss=0, missing=None,\n#              monotone_constraints='()', n_estimators=500, n_jobs=0,\n#              num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1,\n#              scale_pos_weight=1, subsample=1, tree_method='exact',\n#              validate_parameters=1, verbose=2, verbosity=None)#  score = 0.12808","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# modelX.fit(trainX,trainY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predTrain=np.expm1(modelX.predict(trainX))\n# XAcc = mean_squared_error(predTrain, np.expm1(trainY))\n# print (str(XAcc) + ' MSE') #Score= 0.12808","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"manually tunning<br>\nafter having seen the score of the light GBM i wanted to atleaset go below a score of 0.13"},{"metadata":{"trusted":true},"cell_type":"code","source":"modelX=xgboost.XGBRegressor( booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.015, max_delta_step=0, max_depth=4,\n             min_child_weight=5, min_split_loss=0, missing=None,\n             monotone_constraints='()', n_estimators=3000, n_jobs=0,\n             num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1,\n             scale_pos_weight=1, subsample=1, tree_method='exact',\n             validate_parameters=1,  verbosity=None)\nmodelX.fit(trainX,trainY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predTrain=np.expm1(modelX.predict(trainX))\nXAcc = mean_squared_error(predTrain, np.expm1(trainY))\nprint (str(XAcc) + ' MSE') #Score= 0.12670","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot results\nplt.figure(figsize=(12,5))\nsns.distplot((np.expm1(trainY)),bins=30, color='m')\nsns.distplot((predTrain),bins=30, color='g')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forrest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import  RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hypertunning 1<br>\nattention, the hypertunning may take a long time"},{"metadata":{"trusted":true},"cell_type":"code","source":"# modelForrest=RandomForestRegressor()# not regressor\n\n# #Hypertunning    from https://www.kaggle.com/harshkothari21/beginners-notebook-90-accuracy\n# n_estimators = [100,300, 500, 1000]\n# depth = [3,5,10,15]\n# min_split=[2,3,4]\n# min_leaf=[2,3,4]\n# bootstrap = ['True', 'False']\n# verbose = [5]\n\n# hyperparameter = {\n#     'n_estimators': n_estimators,\n#     'max_depth':depth,\n#     'bootstrap':bootstrap,\n#     'verbose':verbose,\n#     'min_samples_split':min_split,\n#     'min_samples_leaf':min_leaf    }\n\n# modelForrest = RandomizedSearchCV(estimator=modelForrest,\n#                                param_distributions=hyperparameter,\n#                                cv=5, #n_iter=100,\n#                                scoring = 'neg_mean_absolute_error',\n#                                n_jobs = 4, \n#                                return_train_score = True,\n#                                random_state=1)\n# modelForrest.fit(trainX,trainY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# modelForrest.best_estimator_    #if best estim are at the edge=> additional tests","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# modelForrest=RandomForestRegressor(bootstrap='False', max_depth=15, min_samples_leaf=3,\n#                       verbose=1)\n# modelForrest.fit(trainX,trainY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predTrain=np.expm1(modelForrest.predict(trainX))\n# ForrestAcc = mean_squared_error(predTrain, np.expm1(trainY))\n# print (str(ForrestAcc) + ' MSE') #Score= 0.14578","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hypertunning 2<br>\nmanually tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"modelForrest=RandomForestRegressor(n_estimators=1100, bootstrap='True', max_depth=14, min_samples_leaf=3,\n                      verbose=1)\nmodelForrest.fit(trainX,trainY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predTrain=np.expm1(modelForrest.predict(trainX))\nForrestAcc = mean_squared_error(predTrain, np.expm1(trainY))\nprint (str(ForrestAcc) + ' MSE') #Score= 0.14492","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot results\nplt.figure(figsize=(12,5))\nsns.distplot((np.expm1(trainY)),bins=30, color='m')\nsns.distplot((predTrain),bins=30, color='g')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Light Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hypertunning 1 attention, the hypertunning may take along time <br>\n(I don't know much about light gbm -> can't hypertune very well)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# modelLight=LGBMRegressor()\n\n# n_estimators = [ 700, 1000, 1500, 2000, 3000]\n# learning_rate=[.02,.05,.1,.15,.20]\n# max_depth =[0,3, 5, 8]\n# num_leaves =[16,25,31]  #Maximum tree leaves\n\n# hyperparameter_grid = {\n#     'n_estimators': n_estimators,\n#     'learning_rate':learning_rate,\n#     'num_leaves':num_leaves,\n#     \"max_depth\": max_depth}\n\n# modelLight = RandomizedSearchCV(estimator=modelLight,\n#             param_distributions=hyperparameter_grid,\n#             cv=5, n_iter=50,\n#             scoring = 'neg_mean_absolute_error',n_jobs = 4,\n#             verbose = 5, \n#             return_train_score = True,\n#             random_state=42)\n# modelLight.fit(trainX,trainY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# modelLight.best_estimator_    #if best estim are at the edge=> additional tests","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# modelLight =LGBMRegressor(learning_rate=0.05, max_depth=3, n_estimators=700, num_leaves=25)\n# modelLight.fit(trainX,trainY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predTrain=np.expm1(modelLight.predict(trainX))\n# LightAcc = mean_squared_error(predTrain, np.expm1(trainY))\n# print (str(LightAcc) + ' MSE') #Score= don't trust this one","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Manual<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# modelLight =LGBMRegressor(learning_rate=0.02, max_depth=4, n_estimators=700*10, num_leaves=12)\n# modelLight.fit(trainX,trainY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predTrain=np.expm1(modelLight.predict(trainX))\n# LightAcc = mean_squared_error(predTrain, np.expm1(trainY))\n# print (str(LightAcc) + ' MSE') #Score= 0.12763","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As I don't know much about light gbm I also looked at another notebook  https://www.kaggle.com/jesucristo/1-house-prices-solution-top-1"},{"metadata":{"trusted":true},"cell_type":"code","source":"modelLight = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=7000,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)\nmodelLight.fit(trainX,trainY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predTrain=np.expm1(modelLight.predict(trainX))\nLightAcc = mean_squared_error(predTrain, np.expm1(trainY))\nprint (str(LightAcc) + ' MSE') #Score= 0.12185","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot results\nplt.figure(figsize=(12,5))\nsns.distplot((np.expm1(trainY)),bins=30, color='m')\nsns.distplot((predTrain),bins=30, color='g')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gradient Boost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import  GradientBoostingRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hypertunning 1\nattention, the hypertunning may take a very long time"},{"metadata":{"trusted":true},"cell_type":"code","source":"# modelGrad=GradientBoostingRegressor()\n\n# n_estimators = [ 700, 1000, 1500, 2000, 3000]\n# loss=[ \"huber\"]\n# criterion=[\"friedman_mse\"]\n# learning_rate=[.02,.05,.1,.15,.20]\n# min_samples_split=[1,3, 5, 8]\n# min_samples_leaf=[1, 2, 3, 4, 5]\n# hyperparameter_grid = {\n#     'n_estimators': n_estimators,\n#     'learning_rate':learning_rate,\n#     'min_samples_split':min_samples_split,\n#     \"loss\" :loss,\n#     'min_samples_leaf':min_samples_leaf,\n#     \"criterion\": criterion}\n\n# modelGrad = RandomizedSearchCV(estimator=modelGrad,\n#             param_distributions=hyperparameter_grid,\n#             cv=5, n_iter=50,\n#             scoring = 'neg_mean_absolute_error',n_jobs = 4,\n#             verbose = 5, \n#             return_train_score = True,\n#             random_state=42)\n# modelGrad.fit(trainX,trainY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# modelGrad.best_estimator_    #if best estim are at the edge=> additional tests","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelGrad=GradientBoostingRegressor(learning_rate=0.02, loss='huber', min_samples_leaf=2,\n                          min_samples_split=5, n_estimators=2000)\nmodelGrad.fit(trainX,trainY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predTrain=np.expm1(modelGrad.predict(trainX))\nGradAcc = mean_squared_error(predTrain, np.expm1(trainY))\nprint (str(GradAcc) + ' MSE') #Score= 0.12964","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"manual tunning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# modelGrad=GradientBoostingRegressor(learning_rate=0.01, loss='huber', min_samples_split=3,\n#                           n_estimators=4000)\n# modelGrad.fit(trainX,trainY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predTrain=np.expm1(modelGrad.predict(trainX))\n# GradAcc = mean_squared_error(predTrain, np.expm1(trainY))\n# print (str(GradAcc) + ' MSE') #Score= 0.13006   did not beat randomized...","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot results\nplt.figure(figsize=(12,5))\nsns.distplot((np.expm1(trainY)),bins=30, color='m')\nsns.distplot((predTrain),bins=30, color='g')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Adaboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import  AdaBoostRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hypertunning 1 <br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# modelAda=AdaBoostRegressor()\n\n# n_estimators = [ 700, 1000, 1500, 2000, 3000]\n# loss=[ \"linear\", \"square\"]\n# learning_rate=[.02,.1,.15,.60, 1]\n\n# hyperparameter_grid = {\n#     'n_estimators': n_estimators,\n#     'learning_rate':learning_rate,\n#     \"loss\" :loss,\n#     }\n\n# modelAda = RandomizedSearchCV(estimator=modelAda,\n#             param_distributions=hyperparameter_grid,\n#             cv=5, n_iter=50,\n#             scoring = 'neg_mean_absolute_error',n_jobs = 4,\n#             verbose = 5, \n#             return_train_score = True,\n#             random_state=42)\n# modelAda.fit(trainX,trainY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# modelAda.best_estimator_    #if best estim are at the edge=> additional tests","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelAda=AdaBoostRegressor(learning_rate=0.02, n_estimators=1000)\nmodelAda.fit(trainX,trainY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predTrain=np.expm1(modelAda.predict(trainX))\nAdaAcc = mean_squared_error(predTrain, np.expm1(trainY))\nprint (str(AdaAcc) + ' MSE') #Score= 0.17866","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"manual tunning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# modelAda=AdaBoostRegressor(learning_rate=0.01, n_estimators=2500)\n# modelAda.fit(trainX,trainY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predTrain=np.expm1(modelAda.predict(trainX))\n# AdaAcc = mean_squared_error(predTrain, np.expm1(trainY))\n# print (str(AdaAcc) + ' MSE') #Score= 0.17873  did not preform better than randomized....","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot results\nplt.figure(figsize=(12,5))\nsns.distplot((np.expm1(trainY)),bins=30, color='m')\nsns.distplot((predTrain),bins=30, color='g')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"AdaBoost, RandomForrest and LinReg performed poorly"},{"metadata":{},"cell_type":"markdown","source":"# Stacking the models"},{"metadata":{"trusted":true},"cell_type":"code","source":"from mlxtend.regressor import StackingCVRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# im testing with dif models\nregressors = (modelLight\n              #,modelLin\n              #,modelForrest\n              ,modelX\n              ,modelSvr \n              #,modelAda \n              #,modelGrad\n)\nmodelStack = StackingCVRegressor(regressors=regressors,\n                                meta_regressor=modelLight,  # i trust xgboost/light gbm the most as they performed better than the other models\n                                use_features_in_secondary=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelStack.fit(trainX,trainY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predTrain=np.expm1(modelStack.predict(trainX))\nStAcc = mean_squared_error(predTrain, np.expm1(trainY))\nprint (str(StAcc) + ' MSE') #Score= ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot results\nplt.figure(figsize=(12,5))\nsns.distplot((np.expm1(trainY)),bins=30, color='m')\nsns.distplot((predTrain),bins=30, color='g')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"notes:\n- 1. try models used: modelForrest, modelX, modelLight, modelAda, modelSvr, modelLin, modelGrad.  Metamodel=modelX, score= 0.12755\n- 2. try models used: modelX, modelLight, modelSvr, modelGrad.  Metamodel=modelX, score=0.12667\n- 3. try models used: modelX, modelLight, modelSvr.  Metamodel=modelX, score=0.12291\n- 4. try models used: modelX, modelLight.  Metamodel=modelX, score=0.13219\n- 5. try models used: modelX, modelLight, modelSvr.  Metamodel=modelLight, score=0.12185\n"},{"metadata":{},"cell_type":"markdown","source":"# averaging\ntaking the average of good performing models to see if results get better."},{"metadata":{"trusted":true},"cell_type":"code","source":"# predictions=pd.DataFrame({'Id': test.index,\n#                        'SalePrice': (np.expm1(modelX.predict(testX))+np.expm1(modelLight.predict(testX)))/2}) # score = 0.12201","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# save Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions=pd.DataFrame({'Id': test.index,\n                       'SalePrice': np.expm1(modelStack.predict(testX))})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot results\nplt.figure(figsize=(12,5))\nsns.distplot(predictions[\"SalePrice\"],bins=30, color='m')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions.to_csv(\"submit.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}